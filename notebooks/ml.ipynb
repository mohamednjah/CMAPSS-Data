{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (3.10.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: lightgbm in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from lightgbm) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\pc\\desktop\\cmapss-data-master\\.venv\\lib\\site-packages (from lightgbm) (1.15.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "%pip install scikit-learn\n",
    "%pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "%pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data loaded successfully!\n",
      "Dataset Shape: (32732, 28)\n",
      "Columns: Index(['engine_id', 'cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3',\n",
      "       'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6',\n",
      "       'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10', 'sensor_11',\n",
      "       'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_16',\n",
      "       'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21',\n",
      "       'dataset_name', 'source'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#  Connect to PostgreSQL & Load Data\n",
    "DB_USER = \"myuser\"\n",
    "DB_PASSWORD = \"mypassword\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"mydatabase\"\n",
    "\n",
    "connection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "query = 'SELECT * FROM preprocessed_data'\n",
    "df = pd.read_sql_query(query, con=engine)\n",
    "\n",
    "print(\"\\n✅ Data loaded successfully!\")\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ RUL and Failure Probability computed successfully!\n",
      "   engine_id  cycle  RUL  failure_probability\n",
      "0          1      1  191                    0\n",
      "1          1      2  190                    0\n",
      "2          1      3  189                    0\n",
      "3          1      4  188                    0\n",
      "4          1      5  187                    0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_8828\\1295633808.py:2: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  df['RUL'] = df.groupby('engine_id')['cycle'].transform(max) - df['cycle']\n"
     ]
    }
   ],
   "source": [
    "# Compute Remaining Useful Life (RUL)\n",
    "df['RUL'] = df.groupby('engine_id')['cycle'].transform(max) - df['cycle']\n",
    "\n",
    "# Optional: Define failure probability (assuming threshold-based classification)\n",
    "df['failure_probability'] = df['RUL'].apply(lambda x: 1 if x <= 20 else 0)  # Example threshold of 20 cycles\n",
    "\n",
    "print(\"\\n✅ RUL and Failure Probability computed successfully!\")\n",
    "print(df[['engine_id', 'cycle', 'RUL', 'failure_probability']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data split into train and test sets.\n",
      "Training set shape: (26185, 26)\n",
      "Test set shape: (6547, 26)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "X = df.drop(columns=['RUL', 'failure_probability','dataset_name',\"source\"], errors='ignore')\n",
    "y_rul = df['RUL']  # Target for RUL prediction\n",
    "y_fail = df['failure_probability'] \n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_rul_train, y_rul_test = train_test_split(X, y_rul, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\n✅ Data split into train and test sets.\")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3184\n",
      "[LightGBM] [Info] Number of data points in the train set: 26185, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 119.091083\n",
      "\n",
      "✅ Models trained successfully!\n",
      "\n",
      "🔹 Random Forest (RUL) Performance:\n",
      "  - MAE: 7.7917\n",
      "  - RMSE: 12.4993\n",
      "  - R² Score: 0.9672\n",
      "  - Accuracy (within 10% tolerance): 67.18%\n",
      "\n",
      "🔹 XGBoost (RUL) Performance:\n",
      "  - MAE: 5.5723\n",
      "  - RMSE: 7.6938\n",
      "  - R² Score: 0.9876\n",
      "  - Accuracy (within 10% tolerance): 77.46%\n",
      "\n",
      "🔹 SVR (RUL) Performance:\n",
      "  - MAE: 27.8765\n",
      "  - RMSE: 39.7768\n",
      "  - R² Score: 0.6679\n",
      "  - Accuracy (within 10% tolerance): 28.00%\n",
      "\n",
      "🔹 LightGBM (RUL) Performance:\n",
      "  - MAE: 31.9470\n",
      "  - RMSE: 40.2227\n",
      "  - R² Score: 0.6604\n",
      "  - Accuracy (within 10% tolerance): 24.82%\n"
     ]
    }
   ],
   "source": [
    "#  SVR\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest \n",
    "rf_rul = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_rul.fit(X_train, y_rul_train)\n",
    "\n",
    "# Train XGBoost \n",
    "xgb_rul = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "xgb_rul.fit(X_train, y_rul_train)\n",
    "\n",
    "# Train SVR\n",
    "svr_rul = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "svr_rul.fit(X_train_scaled, y_rul_train)\n",
    "\n",
    "# Train LightGBM \n",
    "lgb_rul = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.01, random_state=42)\n",
    "lgb_rul.fit(X_train, y_rul_train)\n",
    "\n",
    "print(\"\\n✅ Models trained successfully!\")\n",
    "\n",
    "calculations = {\n",
    "    \"Random Forest (RUL)\": {\n",
    "        \"MAE\": 0.0,\n",
    "        \"RMSE\": 0.0,\n",
    "        \"R² Score\": 0.0,\n",
    "        \"Accuracy\": 0.0\n",
    "    },\n",
    "    \"XGBoost (RUL)\": {\n",
    "        \"MAE\": 0.0,\n",
    "        \"RMSE\": 0.0,\n",
    "        \"R² Score\": 0.0,\n",
    "        \"Accuracy\": 0.0\n",
    "    },\n",
    "    \"SVR (RUL)\": {\n",
    "        \"MAE\": 0.0,\n",
    "        \"RMSE\": 0.0,\n",
    "        \"R² Score\": 0.0,\n",
    "        \"Accuracy\": 0.0\n",
    "    },\n",
    "    \"LightGBM (RUL)\": {\n",
    "        \"MAE\": 0.0,\n",
    "        \"RMSE\": 0.0,\n",
    "        \"R² Score\": 0.0,\n",
    "        \"Accuracy\": 0.0\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def regression_accuracy(y_true, y_pred, tolerance=0.1):\n",
    "    \n",
    "    within_tolerance = abs(y_true - y_pred) / y_true <= tolerance\n",
    "    return within_tolerance.mean()\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n🔹 {model_name} Performance:\")\n",
    "    print(f\"  - MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "    calculations[model_name][\"MAE\"] = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred)) \n",
    "    print(f\"  - RMSE: {rmse:.4f}\")\n",
    "    print(f\"  - R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "    calculations[model_name][\"RMSE\"] = rmse\n",
    "    calculations[model_name][\"R² Score\"] = r2_score(y_test, y_pred)\n",
    "\n",
    "    accuracy = regression_accuracy(y_test, y_pred)\n",
    "    print(f\"  - Accuracy (within 10% tolerance): {accuracy:.2%}\")\n",
    "    calculations[model_name][\"Accuracy\"] = accuracy\n",
    "    return y_pred\n",
    "\n",
    "# Evaluate models on test data\n",
    "y_rul_pred_rf = evaluate_model(rf_rul, X_test, y_rul_test, \"Random Forest (RUL)\")\n",
    "y_rul_pred_xgb = evaluate_model(xgb_rul, X_test, y_rul_test, \"XGBoost (RUL)\")\n",
    "y_rul_pred_svr = evaluate_model(svr_rul, X_test_scaled, y_rul_test, \"SVR (RUL)\")\n",
    "y_rul_pred_lgb = evaluate_model(lgb_rul, X_test, y_rul_test, \"LightGBM (RUL)\")                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model results successfully stored in PostgreSQL table 'trainning_data_results'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Step 5: Define database connection\n",
    "DB_USER = \"myuser\"\n",
    "DB_PASSWORD = \"mypassword\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"mydatabase\"\n",
    "\n",
    "# Construct the connection string\n",
    "connection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "\n",
    "# Step 6: Save preprocessed data to PostgreSQL\n",
    "table_name = \"trainning_data_results\"\n",
    "\n",
    "model_results = [\n",
    "    {\n",
    "        \"model\": \"Random Forest (RUL)\",\n",
    "        \"calculations\": json.dumps({k: float(v) if isinstance(v, np.float64) else v for k, v in calculations[\"Random Forest (RUL)\"].items()}),  # Convert np.float64 to standard float\n",
    "        \"predictions\": json.dumps(y_rul_pred_rf.tolist())  # Convert list to JSON\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"XGBoost (RUL)\",\n",
    "        \"calculations\": json.dumps({k: float(v) if isinstance(v, np.float64) else v for k, v in calculations[\"XGBoost (RUL)\"].items()}),\n",
    "        \"predictions\": json.dumps(y_rul_pred_xgb.tolist())\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"SVR (RUL)\",\n",
    "        \"calculations\": json.dumps({k: float(v) if isinstance(v, np.float64) else v for k, v in calculations[\"SVR (RUL)\"].items()}),\n",
    "        \"predictions\": json.dumps(y_rul_pred_svr.tolist())\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"LightGBM (RUL)\",\n",
    "        \"calculations\": json.dumps({k: float(v) if isinstance(v, np.float64) else v for k, v in calculations[\"LightGBM (RUL)\"].items()}),\n",
    "        \"predictions\": json.dumps(y_rul_pred_lgb.tolist())\n",
    "    }\n",
    "]\n",
    "df_model_results = pd.DataFrame(model_results)\n",
    "\n",
    "# Save the preprocessed data to the database\n",
    "df_model_results.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"✅ Model results successfully stored in PostgreSQL table '{table_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ewma_{i}'] = df[sensor_col].ewm(span=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_mean_{i}'] = df[sensor_col].rolling(window=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_std_{i}'] = df[sensor_col].rolling(window=window_size).std()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_1_{i}'] = df[sensor_col].shift(1)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_2_{i}'] = df[sensor_col].shift(2)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_3_{i}'] = df[sensor_col].shift(3)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ewma_{i}'] = df[sensor_col].ewm(span=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_mean_{i}'] = df[sensor_col].rolling(window=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_std_{i}'] = df[sensor_col].rolling(window=window_size).std()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_1_{i}'] = df[sensor_col].shift(1)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_2_{i}'] = df[sensor_col].shift(2)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_3_{i}'] = df[sensor_col].shift(3)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ewma_{i}'] = df[sensor_col].ewm(span=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_mean_{i}'] = df[sensor_col].rolling(window=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_std_{i}'] = df[sensor_col].rolling(window=window_size).std()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_1_{i}'] = df[sensor_col].shift(1)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_2_{i}'] = df[sensor_col].shift(2)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_3_{i}'] = df[sensor_col].shift(3)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ewma_{i}'] = df[sensor_col].ewm(span=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_mean_{i}'] = df[sensor_col].rolling(window=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_std_{i}'] = df[sensor_col].rolling(window=window_size).std()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_1_{i}'] = df[sensor_col].shift(1)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_2_{i}'] = df[sensor_col].shift(2)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_3_{i}'] = df[sensor_col].shift(3)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ewma_{i}'] = df[sensor_col].ewm(span=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_mean_{i}'] = df[sensor_col].rolling(window=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rolling_std_{i}'] = df[sensor_col].rolling(window=window_size).std()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_1_{i}'] = df[sensor_col].shift(1)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_2_{i}'] = df[sensor_col].shift(2)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'lag_3_{i}'] = df[sensor_col].shift(3)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ewma_{i}'] = df[sensor_col].ewm(span=window_size).mean()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_16556\\3274456678.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['sensor_1_2_interaction'] = df['sensor_1'] * df['sensor_2']\n"
     ]
    }
   ],
   "source": [
    "def create_time_series_features(df, window_size=10):\n",
    "    for i in range(1, 22):  # Loop from sensor_1 to sensor_21\n",
    "        sensor_col = f'sensor_{i}'\n",
    "        \n",
    "        # Rolling Mean & Std\n",
    "        df[f'rolling_mean_{i}'] = df[sensor_col].rolling(window=window_size).mean()\n",
    "        df[f'rolling_std_{i}'] = df[sensor_col].rolling(window=window_size).std()\n",
    "        \n",
    "        # Lags\n",
    "        df[f'lag_1_{i}'] = df[sensor_col].shift(1)\n",
    "        df[f'lag_2_{i}'] = df[sensor_col].shift(2)\n",
    "        df[f'lag_3_{i}'] = df[sensor_col].shift(3)\n",
    "        \n",
    "        # Exponential Weighted Moving Average (EWMA)\n",
    "        df[f'ewma_{i}'] = df[sensor_col].ewm(span=window_size).mean()\n",
    "    \n",
    "    # Interaction Features (Example: sensor_1 * sensor_2)\n",
    "    df['sensor_1_2_interaction'] = df['sensor_1'] * df['sensor_2']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "data = create_time_series_features(df)\n",
    "\n",
    "# Drop NaN values caused by rolling & lag features\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rul = data['RUL']  # Target for RUL prediction\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_rul_train, y_rul_test = train_test_split(X, y_rul, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n✅ Data split into train and test sets.\")\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3184\n",
      "[LightGBM] [Info] Number of data points in the train set: 26185, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 119.091083\n",
      "\n",
      "✅ Models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "rf_rul = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_rul.fit(X_train, y_rul_train)\n",
    "\n",
    "    # Train XGBoost\n",
    "xgb_rul = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "xgb_rul.fit(X_train, y_rul_train)\n",
    "\n",
    "    # Train SVR\n",
    "svr_rul = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "svr_rul.fit(X_train, y_rul_train)\n",
    "\n",
    "    # Train LightGBM\n",
    "lgb_rul = LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "lgb_rul.fit(X_train, y_rul_train)\n",
    "\n",
    "print(\"\\n✅ Models trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_accuracy(y_true, y_pred, tolerance=0.1):\n",
    "    \"\"\"Calculates percentage of predictions within 10% tolerance.\"\"\"\n",
    "    within_tolerance = abs(y_true - y_pred) / y_true <= tolerance\n",
    "    return within_tolerance.mean()\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluates model performance.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n🔹 {model_name} Performance:\")\n",
    "    print(f\"  - MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"  - RMSE: {rmse:.4f}\")\n",
    "    print(f\"  - R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "    accuracy = regression_accuracy(y_test, y_pred)\n",
    "    print(f\"  - Accuracy (within 10% tolerance): {accuracy:.2%}\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Random Forest (RUL) Performance:\n",
      "  - MAE: 7.7917\n",
      "  - RMSE: 12.4993\n",
      "  - R² Score: 0.9672\n",
      "  - Accuracy (within 10% tolerance): 67.18%\n",
      "\n",
      "🔹 XGBoost (RUL) Performance:\n",
      "  - MAE: 5.5723\n",
      "  - RMSE: 7.6938\n",
      "  - R² Score: 0.9876\n",
      "  - Accuracy (within 10% tolerance): 77.46%\n",
      "\n",
      "🔹 SVR (RUL) Performance:\n",
      "  - MAE: 21.7412\n",
      "  - RMSE: 36.8363\n",
      "  - R² Score: 0.7152\n",
      "  - Accuracy (within 10% tolerance): 51.75%\n",
      "\n",
      "🔹 LightGBM (RUL) Performance:\n",
      "  - MAE: 7.4456\n",
      "  - RMSE: 9.8617\n",
      "  - R² Score: 0.9796\n",
      "  - Accuracy (within 10% tolerance): 72.28%\n"
     ]
    }
   ],
   "source": [
    "y_rul_pred_rf = evaluate_model(rf_rul, X_test, y_rul_test, \"Random Forest (RUL)\")\n",
    "y_rul_pred_xgb = evaluate_model(xgb_rul, X_test, y_rul_test, \"XGBoost (RUL)\")\n",
    "y_rul_pred_svr = evaluate_model(svr_rul, X_test, y_rul_test, \"SVR (RUL)\")\n",
    "y_rul_pred_lgb = evaluate_model(lgb_rul, X_test, y_rul_test, \"LightGBM (RUL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LSTM for RUL Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>engine_id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>op_setting_1</th>\n",
       "      <th>op_setting_2</th>\n",
       "      <th>op_setting_3</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_2_20</th>\n",
       "      <th>lag_3_20</th>\n",
       "      <th>ewma_20</th>\n",
       "      <th>rolling_mean_21</th>\n",
       "      <th>rolling_std_21</th>\n",
       "      <th>lag_1_21</th>\n",
       "      <th>lag_2_21</th>\n",
       "      <th>lag_3_21</th>\n",
       "      <th>ewma_21</th>\n",
       "      <th>sensor_1_2_interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170588</td>\n",
       "      <td>0.463756</td>\n",
       "      <td>0.307394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643411</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.657150</td>\n",
       "      <td>0.689492</td>\n",
       "      <td>0.062431</td>\n",
       "      <td>0.713152</td>\n",
       "      <td>0.579541</td>\n",
       "      <td>0.672512</td>\n",
       "      <td>0.696860</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.265511</td>\n",
       "      <td>0.310432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705426</td>\n",
       "      <td>0.643411</td>\n",
       "      <td>0.649593</td>\n",
       "      <td>0.697801</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>0.800557</td>\n",
       "      <td>0.713152</td>\n",
       "      <td>0.579541</td>\n",
       "      <td>0.720688</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.273529</td>\n",
       "      <td>0.300188</td>\n",
       "      <td>0.302161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.705426</td>\n",
       "      <td>0.662297</td>\n",
       "      <td>0.689784</td>\n",
       "      <td>0.072741</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.800557</td>\n",
       "      <td>0.713152</td>\n",
       "      <td>0.707893</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.570588</td>\n",
       "      <td>0.274702</td>\n",
       "      <td>0.313639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620155</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.652504</td>\n",
       "      <td>0.680251</td>\n",
       "      <td>0.086862</td>\n",
       "      <td>0.656646</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.800557</td>\n",
       "      <td>0.673168</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.358824</td>\n",
       "      <td>0.499478</td>\n",
       "      <td>0.285449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.620155</td>\n",
       "      <td>0.682241</td>\n",
       "      <td>0.681461</td>\n",
       "      <td>0.086751</td>\n",
       "      <td>0.530967</td>\n",
       "      <td>0.656646</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.674441</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    engine_id  cycle  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
       "9           1     10       -0.0033        0.0001         100.0       0.0   \n",
       "10          1     11        0.0018       -0.0003         100.0       0.0   \n",
       "11          1     12        0.0016        0.0002         100.0       0.0   \n",
       "12          1     13       -0.0019        0.0004         100.0       0.0   \n",
       "13          1     14        0.0009        0.0000         100.0       0.0   \n",
       "\n",
       "    sensor_2  sensor_3  sensor_4  sensor_5  ...  lag_2_20  lag_3_20   ewma_20  \\\n",
       "9   0.170588  0.463756  0.307394       0.0  ...  0.643411  0.744186  0.657150   \n",
       "10  0.338235  0.265511  0.310432       0.0  ...  0.705426  0.643411  0.649593   \n",
       "11  0.273529  0.300188  0.302161       0.0  ...  0.627907  0.705426  0.662297   \n",
       "12  0.570588  0.274702  0.313639       0.0  ...  0.620155  0.627907  0.652504   \n",
       "13  0.358824  0.499478  0.285449       0.0  ...  0.713178  0.620155  0.682241   \n",
       "\n",
       "    rolling_mean_21  rolling_std_21  lag_1_21  lag_2_21  lag_3_21   ewma_21  \\\n",
       "9          0.689492        0.062431  0.713152  0.579541  0.672512  0.696860   \n",
       "10         0.697801        0.073100  0.800557  0.713152  0.579541  0.720688   \n",
       "11         0.689784        0.072741  0.813500  0.800557  0.713152  0.707893   \n",
       "12         0.680251        0.086862  0.656646  0.813500  0.800557  0.673168   \n",
       "13         0.681461        0.086751  0.530967  0.656646  0.813500  0.674441   \n",
       "\n",
       "    sensor_1_2_interaction  \n",
       "9                      0.0  \n",
       "10                     0.0  \n",
       "11                     0.0  \n",
       "12                     0.0  \n",
       "13                     0.0  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine_id                   int64\n",
      "cycle                       int64\n",
      "op_setting_1              float64\n",
      "op_setting_2              float64\n",
      "op_setting_3              float64\n",
      "                           ...   \n",
      "lag_1_21                  float64\n",
      "lag_2_21                  float64\n",
      "lag_3_21                  float64\n",
      "ewma_21                   float64\n",
      "sensor_1_2_interaction    float64\n",
      "Length: 153, dtype: object\n",
      "    engine_id  cycle  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
      "9           1     10       -0.0033        0.0001         100.0       0.0   \n",
      "10          1     11        0.0018       -0.0003         100.0       0.0   \n",
      "11          1     12        0.0016        0.0002         100.0       0.0   \n",
      "12          1     13       -0.0019        0.0004         100.0       0.0   \n",
      "13          1     14        0.0009        0.0000         100.0       0.0   \n",
      "\n",
      "    sensor_2  sensor_3  sensor_4  sensor_5  ...  lag_2_20  lag_3_20   ewma_20  \\\n",
      "9   0.170588  0.463756  0.307394       0.0  ...  0.643411  0.744186  0.657150   \n",
      "10  0.338235  0.265511  0.310432       0.0  ...  0.705426  0.643411  0.649593   \n",
      "11  0.273529  0.300188  0.302161       0.0  ...  0.627907  0.705426  0.662297   \n",
      "12  0.570588  0.274702  0.313639       0.0  ...  0.620155  0.627907  0.652504   \n",
      "13  0.358824  0.499478  0.285449       0.0  ...  0.713178  0.620155  0.682241   \n",
      "\n",
      "    rolling_mean_21  rolling_std_21  lag_1_21  lag_2_21  lag_3_21   ewma_21  \\\n",
      "9          0.689492        0.062431  0.713152  0.579541  0.672512  0.696860   \n",
      "10         0.697801        0.073100  0.800557  0.713152  0.579541  0.720688   \n",
      "11         0.689784        0.072741  0.813500  0.800557  0.713152  0.707893   \n",
      "12         0.680251        0.086862  0.656646  0.813500  0.800557  0.673168   \n",
      "13         0.681461        0.086751  0.530967  0.656646  0.813500  0.674441   \n",
      "\n",
      "    sensor_1_2_interaction  \n",
      "9                      0.0  \n",
      "10                     0.0  \n",
      "11                     0.0  \n",
      "12                     0.0  \n",
      "13                     0.0  \n",
      "\n",
      "[5 rows x 153 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_time_series_data(df, window_size=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - window_size):\n",
    "        X.append(df.iloc[i : i + window_size, :-1].values)  # Features\n",
    "        y.append(df.iloc[i + window_size, -1])  # Target (RUL)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Drop NaN values (after rolling/lag features)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "features = df.drop(columns=['RUL', 'failure_probability', 'dataset_name'], errors='ignore')\n",
    "target = df['RUL']\n",
    "\n",
    "print(features.dtypes)  # Check data types of all columns\n",
    "print(features.head())  # Look at first few rows\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':  # If column contains strings\n",
    "        print(f\"⚠ Column '{col}' contains non-numeric values.\")\n",
    "        print(df[col].unique())  # Print unique values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 38.2 MiB for an array with shape (153, 32723) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m df_scaled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRUL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Add back target\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create time-series dataset\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_time_series_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 10 time steps\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train-test split (80-20)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m, in \u001b[0;36mcreate_time_series_data\u001b[1;34m(df, window_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m X, y \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m-\u001b[39m window_size):\n\u001b[1;32m----> 4\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues)  \u001b[38;5;66;03m# Features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(df\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m+\u001b[39m window_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Target (RUL)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1694\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_lowerdim(tup)\n\u001b[1;32m-> 1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1020\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1020\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1729\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m   1724\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame indexer is not allowed for .iloc\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using .loc for automatic alignment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1726\u001b[0m     )\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m-> 1729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_slice_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   1732\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1765\u001b[0m, in \u001b[0;36m_iLocIndexer._get_slice_axis\u001b[1;34m(self, slice_obj, axis)\u001b[0m\n\u001b[0;32m   1763\u001b[0m labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1764\u001b[0m labels\u001b[38;5;241m.\u001b[39m_validate_positional_slice(slice_obj)\n\u001b[1;32m-> 1765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4369\u001b[0m, in \u001b[0;36mNDFrame._slice\u001b[1;34m(self, slobj, axis)\u001b[0m\n\u001b[0;32m   4367\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slobj, \u001b[38;5;28mslice\u001b[39m), \u001b[38;5;28mtype\u001b[39m(slobj)\n\u001b[0;32m   4368\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis)\n\u001b[1;32m-> 4369\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4370\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   4371\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32minternals.pyx:869\u001b[0m, in \u001b[0;36mpandas._libs.internals.BlockManager.get_slice\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pc\\Desktop\\CMAPSS-Data-master\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 38.2 MiB for an array with shape (153, 32723) and data type float64"
     ]
    }
   ],
   "source": [
    "# Convert back to DataFrame\n",
    "df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_scaled['RUL'] = target.values  # Add back target\n",
    "\n",
    "# Create time-series dataset\n",
    "X, y = create_time_series_data(df_scaled, window_size=5)  # 10 time steps\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape for LSTM (samples, time_steps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)  # Output layer for RUL prediction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Save model\n",
    "model.save(\"lstm_rul_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n🔹 LSTM Performance:\")\n",
    "print(f\"  - MAE: {mae:.4f}\")\n",
    "print(f\"  - RMSE: {rmse:.4f}\")\n",
    "print(f\"  - R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_fail' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_fail \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(rf_fail, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf_fail_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(\u001b[43mxgb_fail\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb_fail_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Models saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#  Store predictions in PostgreSQL\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_fail' is not defined"
     ]
    }
   ],
   "source": [
    "# Save trained models\n",
    "joblib.dump(rf_rul, \"rf_rul_model.pkl\")\n",
    "joblib.dump(xgb_rul, \"xgb_rul_model.pkl\")\n",
    "if y_fail is not None:\n",
    "    joblib.dump(rf_fail, \"rf_fail_model.pkl\")\n",
    "    joblib.dump(xgb_fail, \"xgb_fail_model.pkl\")\n",
    "\n",
    "print(\"\\n✅ Models saved successfully!\")\n",
    "\n",
    "#  Store predictions in PostgreSQL\n",
    "df_test = X_test.copy()\n",
    "df_test['RUL_Prediction_RF'] = y_rul_pred_rf\n",
    "df_test['RUL_Prediction_XGB'] = y_rul_pred_xgb\n",
    "\n",
    "if y_fail is not None:\n",
    "    df_test['Failure_Prob_Prediction_RF'] = y_fail_pred_rf\n",
    "    df_test['Failure_Prob_Prediction_XGB'] = y_fail_pred_xgb\n",
    "\n",
    "df_test.to_sql(\"predicted_results\", engine, if_exists=\"replace\", index=False)\n",
    "print(\"\\n✅ Predictions stored in PostgreSQL.\")\n",
    "\n",
    "#  Display sample predictions\n",
    "print(\"\\n🔍 Sample Predictions:\")\n",
    "print(df_test[['RUL_Prediction_RF', 'RUL_Prediction_XGB']].head())\n",
    "\n",
    "if y_fail is not None:\n",
    "    print(df_test[['Failure_Prob_Prediction_RF', 'Failure_Prob_Prediction_XGB']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table name\n",
    "result_table = \"ml_predictions\"\n",
    "\n",
    "# Save predictions to PostgreSQL\n",
    "df_results.to_sql(result_table, engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"✅ ML predictions saved to PostgreSQL table '{result_table}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
